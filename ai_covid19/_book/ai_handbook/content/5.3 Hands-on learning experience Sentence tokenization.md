# 5.3 Hands-on learning experience: Sentence tokenization ![13](https://img.shields.io/badge/Age-13%2B-9cf)

When we talk about the natural language processing, we can learn how to handle the sentence tokenization and formulate document vector firstly. Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here seems very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark. However, even in English, this problem is not trivial due to the use of full stop character for abbreviations. When processing plain text, tables of abbreviations that contain periods can help us to prevent incorrect assignment of sentence boundaries. In many cases, we use libraries to do that job for us, so don’t worry too much for the details for now.

For example, let’s look a piece of text about a famous board game called backgammon. It's *“Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two-player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.”*

To apply a sentence tokenization with NLTK we can use the nltk.sent_tokenize function.

```python
text = "Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two-player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice."
sentences = nltk.sent_tokenize(text)
for sentence in sentences:
    print(sentence)
    print()
```

As an output, we get the three component sentences separately.

```python
Backgammon is one of the oldest known board games.
Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.
It is a two-player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.
```

Accordingly, we can learn how to process bag-of-word and count the document vector. The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document.
To use this model, we need to:

1. Design a vocabulary of known words (also called tokens)

2. Choose a measure of the presence of known words

Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document. The intuition is that similar documents have similar contents. Also, via the content, we can learn something about the meaning of the document. There are two steps.

**1. Load the Data**

We have a reviews.txt file, it is our data and we want to load it as an array. The file content is:

> I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.

To achieve this we can simply read the file and split it by lines.

```python
with open("simple movie reviews.txt", "r") as file:
    documents = file.read().splitlines()
print(documents)
```

The output is

```bash
'I like this movie, it's funny.', 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.'
```

**2. Design the Vocabulary**

Let’s get all the unique words from the four loaded sentences ignoring the case, punctuation, and one-character tokens. These words will be our vocabulary (known words). Thus, we need to score the words in each document. The task here is to convert each raw text into a vector of numbers. The simplest scoring method is to mark the presence of words with one for present and zero for absence.

Now, let’s see how we can create a bag-of-words model using the mentioned above CountVectorizer class.

```python
# Import the libraries we need
from sklearn.feature_extraction.text
import CountVectorizerimport pandas as pd

# Step 2. Design the Vocabulary
# The default token pattern removes tokens of a single character. That's why we don't have the "I" and "s" tokens in the output
count_vectorizer = CountVectorizer()

# Step 3. Create the Bag-of-Words Model
bag_of_words = count_vectorizer.fit_transform(documents)

# Show the Bag-of-Words Model as a pandas DataFrame
feature_names = count_vectorizer.get_feature_names()
pd.DataFrame(bag_of_words.toarray(), columns = feature_names)
```

Our output is shown as follows,

![5.3](https://md.hass.live/5.3.png)

We can compare to our text content, this learning can be denoted as the basic introduction of natural language processing.

> I like this movie, it's funny.I hate this movie.This was awesome! I like it.Nice one. I love it.

#### Test

Test the code below with username `yuanzhuo` and password `yuanzhuo`. Please choose kernel `conda-yuanzhuo`. Practice more on [Yuanzhuo Online Code Platform](https://code.yuanzhuo.bnu.edu.cn/)

<iframe src="https://code.yuanzhuo.bnu.edu.cn/user/yuanzhuo/notebooks/AI%20＆%20COVID-19%20Handbook/Chap5/Sentence.ipynb" width="100%" height="980" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<iframe src="https://code.yuanzhuo.bnu.edu.cn/user/yuanzhuo/notebooks/AI%20＆%20COVID-19%20Handbook/Chap5/Bag-of-words.ipynb" width="100%" height="980" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

#### Quiz

{%mcq ans="o3", random=true%}
{%title%}
Please select the appropriate example that utilizes Natural Language Process (NLP) in fighting COVID-19 pandemic.
{%o1%} Administrators of Twitter are deleting tweets irrelevant to coronavirus.
{%o2%} Alexa telling traffic conditions with voice inputs.
{%o3%} Chatbots answering patients’ inquiries with various input such as voice and text.
{%o4%} AI technology diagnosing patients’ symptoms and predicting how severe they are by collecting the data of their chest CT.
<!-- {%hint%} Try Again ... -->
{%endmcq%}

{%mcq ans="o1", random=true%}
{%title%}
What are the main components of Natural Language Process (NLP)?
{%o1%} Natural Language Understanding and Natural Language Generation.
{%o2%} Natural Language Pairing and Natural Language Converting.
{%o3%} Natural Language Understanding and Natural Language Pairing.
{%o4%} Natural Language Generation and Natural Language Converting.
<!-- {%hint%} Try Again ... -->
{%endmcq%}
