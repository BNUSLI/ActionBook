# 3.3 Hands-on learning experience: Reasoning using decision trees ![11](https://img.shields.io/badge/Age-11%2B-blueviolet)

Herein, we provided the basic solution to process the data from data web site. The method is to make the decision by decision tree. What is a decision tree?  

| **Decision (/dɪˈsɪʒən/) Tree (/tri/)**
| --
| *Decision trees are one of the most popular and powerful machine learning algorithms. It is used to determine a set of actions or show a statistical probability. Each branch of the decision tree represents a possible decision, outcome, or reaction. The farthest branches on the tree represent the end results.*

One of the reasons why decision trees are so powerful is that they can be easily visualized so that a human can understand what is going on. Imagine a flowchart, where each level is a question with a yes or no answer. Eventually an answer will give you a solution to the initial problem. That is a decision tree. Everybody subconsciously uses decision trees all the time for most menial tasks. Decision trees in machine learning take that ability and multiply it to be able to artificially perform complex decision-making tasks.

The decision tree analyses a dataset in order to construct a set of rules, or questions, which are used to predict a class. Let us consider a dataset consisting of lots of different animals and some of their characteristics. These characteristics can be used to predict their class. If we take a falcon and a tiger, a question that would split these two animals would be ‘Does this animal have feathers?’ or perhaps ‘Does this animal lay eggs?’. The answer no for either of these questions would lead to the classification of a tiger, whereas yes would be a falcon. These rules can be built up to create a model that can classify complex situations. To extend the animal classification example, consider the scenario of needing to classify a selection of animals into mammals, reptile, or insects. Look at the visualized decision tree (see Figure 17) to see how two simple questions can be used to split the data. These simple questions layered one after another, allow the classification of a wide range of animals. This is the power of decision trees. Now if we give the trained decision tree a new animal, for example a snake, it will classify it. Does a snake have vertebra? Yes. Is a snake cold-blooded? Yes. Therefore, the model will classify it as a reptile, the correct answer!

<br>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://md.hass.live/ai17.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 1px;">Figure 17. Decision tree to classify animals into mammals, reptile, or insects</div>
</center>
<br>

When a human construct a decision tree, the questions and answers are based on their logic and knowledge. In data science the creation of these rules is usually governed by an algorithm learning which questions to ask by analyzing the entire dataset. To put this into context we will come back to the animal example, the algorithm will look at all the animals to figure out that all animals that do not breathe air are all fish. This mathematically splits the dataset by its class. This creates powerful algorithms that can classify new data into classes in a way that any human can understand. Decision trees can become much more powerful when used as ensembles. Ensembles are clever ways of combining decision trees to create a more powerful model. These ensembles create state of the art machine learning algorithms that can outperform neural networks in some cases. The two most popular ensemble techniques are random forests and gradient boosting.

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO
from IPython.display import Image
from pydot import graph_from_dot_data
import pandas as pd
import numpy as np
```

Accordingly, Let's take a look at how we could go about implementing a decision tree classifier in Python. For example, we’ll be working with what has to be the most popular dataset in the field of machine learning, the iris dataset from UC Irvine Machine Learning Repository. The following source codes are parts of full version. Besides, we used the following library as the handy tool.

```python
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Categorical.from_codes(iris.target, iris.target_names)
```

In the proceeding section, we will attempt to build a decision tree classifier to determine the kind of flower given its dimensions.

```python
X.head()
```

![3.3](https://md.hass.live/3.3.png)

Although, decision trees can handle categorical data, we still encode the targets in terms of digits (i.e. setosa=0, versicolor=1, virginica=2) in order to create a confusion matrix at a later point. Fortunately, the pandas library provides a method for this very purpose.

```python
y = pd.get_dummies(y)
```

We’ll want to evaluate the performance of our model. Therefore, we set a quarter of the data aside for testing.

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
```

Next, we create and train an instance of the DecisionTreeClassifer class. We provide the y values because our model using a supervised machine learning algorithm.

```python
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
```

We can view the actual decision tree produced by our model by running the following block of code.

```python
dot_data = StringIO()
export_graphviz(dt, out_file=dot_data, feature_names=iris.feature_names)
(graph, ) = graph_from_dot_data(dot_data.getvalue()) Image(graph.create_png())
```

Notice how it provides the Gini impurity, the total number of samples, the classification criteria and the number of samples on the left/right sides.

#### Test

Test the code below with username `yuanzhuo` and password `yuanzhuo`. Practice more on [Yuanzhuo Online Code Platform](https://code.yuanzhuo.bnu.edu.cn/)

<iframe src="https://code.yuanzhuo.bnu.edu.cn/user/yuanzhuo/notebooks/AI%20＆%20COVID-19%20Handbook/Chap3/decisionTree.ipynb" width="100%" height="980" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

#### Tip

Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set. If you have interesting, please refer the following address.<https://bambielli.com/til/2017-10-29-gini-impurity>
